
# base_ckpt_path: cache/savedmodels/synthetic-oscar.ckpt

trainer:
  max_epochs: 1000
  gradient_clip_val: 4.0
  gradient_clip_algorithm: "norm"
  logger:
    - class_path: lightning.pytorch.loggers.NeptuneLogger
      init_args:
        project: isears/ptbxlae
        log_model_checkpoints: False
      dict_kwargs:
        tags:
          - synthetic-finetuning
          - full-10s
          - multichannel
          - DTWLoss
  callbacks:
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        patience: 5
        monitor: val/loss
    - class_path: ptbxlae.modeling.NeptuneUploadingModelCheckpoint
      init_args:
        save_top_k: 1
        monitor: val/loss
        mode: min
        dirpath: ./cache/savedmodels/
        filename: synthetic-finetune-{epoch:03d}-{val/loss:.6f}

model:
  class_path: ptbxlae.modeling.convolutionalVAE.ConvolutionalEcgVAE
  init_args:
    latent_dim: 100
    conv_depth: 3
    fc_depth: 2
    kernel_size: 7
    dropout: null
    batchnorm: False
    lr: 4.7491754446145785e-05
    n_channels: 12
    seq_len: 1000
    loss:
      class_path: ptbxlae.modeling.SumReducingSoftDTWLoss
      init_args:
        gamma: 1
data:
  class_path: ptbxlae.dataprocessing.dataModules.PtbxlCleanDM
  init_args:
    batch_size: 32
    lowres: True